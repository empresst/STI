{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9BmpvFbSiR7",
        "outputId": "2ccfd4d6-cae4-42c2-ac6a-f7b6bff5c6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDr15aQEz_L1",
        "outputId": "671fda69-b2cd-4fcb-9dd2-375216a4a29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.9.1 in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.11.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkGG2e3PgvAe",
        "outputId": "59ba6bc9-2199-46fc-eb72-67867411e6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           V1          V2  V3     V4   V5     V6\n",
            "319001000000005001 2012-01-01  00:00:00     1  37.98  0.0  38.34\n",
            "319001000000005001 2012-01-01  00:15:00     2  41.94  0.0  42.54\n",
            "319001000000005001 2012-01-01  00:30:00     3  38.16  0.0  37.68\n",
            "319001000000005001 2012-01-01  00:45:00     4  38.64  0.0  39.00\n",
            "319001000000005001 2012-01-01  01:00:00     5  39.00  0.0  39.12\n",
            "319001000000005001 2012-01-01  01:15:00     6  37.44  0.0  36.48\n",
            "319001000000005001 2012-01-01  01:30:00     7  39.00  0.0  40.08\n",
            "319001000000005001 2012-01-01  01:45:00     8  40.92  0.0  41.64\n",
            "319001000000005001 2012-01-01  02:00:00     9   1.32  0.0   1.20\n",
            "319001000000005001 2012-01-01  02:15:00    10   1.20  0.0   0.96\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/G1.txt', delimiter=';')\n",
        "#data = pd.read_csv('/content/drive/MyDrive/household_power_consumptionNew.csv')\n",
        "data1 = data\n",
        "df2 = pd.DataFrame(data1)\n",
        "\n",
        "data['V1'] = pd.to_datetime(data['V1'])\n",
        "print(data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMeQwjO_5_25",
        "outputId": "35e989c4-8b30-41fe-bb21-1054308444c3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "319001000000005001   2012-12-31\n",
            "319001000000005001   2012-12-31\n",
            "319001000000005001   2012-12-31\n",
            "319001000000005001   2012-12-31\n",
            "319001000000005001   2012-12-31\n",
            "Name: V1, dtype: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(data)\n",
        "print(df['V1'].tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCMFejacojKR",
        "outputId": "5dbf0b3a-ed57-4f20-9450-780781f0da90",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V1    datetime64[ns]\n",
            "V2            object\n",
            "V3             int64\n",
            "V4           float64\n",
            "V5           float64\n",
            "V6           float64\n",
            "dtype: object\n",
            "                           V1          V2     V3     V4   V5     V6\n",
            "319001000000005001 2012-01-01  00:00:00        1  37.98  0.0  38.34\n",
            "319001000000005001 2012-01-01  00:15:00        2  41.94  0.0  42.54\n",
            "319001000000005001 2012-01-01  00:30:00        3  38.16  0.0  37.68\n",
            "319001000000005001 2012-01-01  00:45:00        4  38.64  0.0  39.00\n",
            "319001000000005001 2012-01-01  01:00:00        5  39.00  0.0  39.12\n",
            "...                       ...         ...    ...    ...  ...    ...\n",
            "319001000000005001 2012-12-31  22:45:00    35132  49.56  0.0  48.72\n",
            "319001000000005001 2012-12-31  23:00:00    35133  51.78  0.0  51.48\n",
            "319001000000005001 2012-12-31  23:15:00    35134  47.58  0.0  47.22\n",
            "319001000000005001 2012-12-31  23:30:00    35135  50.76  0.0  50.76\n",
            "319001000000005001 2012-12-31  23:45:00    35136  41.52  0.0  40.86\n",
            "\n",
            "[35136 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(data)\n",
        "\n",
        "data_types = df.dtypes\n",
        "\n",
        "print(data_types)\n",
        "df['V1'] = pd.to_datetime(df['V1'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-bBjXeR7nZR"
      },
      "outputs": [],
      "source": [
        "data['year'] = data['V1'].apply(lambda x: x.year)\n",
        "data['quarter'] = data['V1'].apply(lambda x: x.quarter)\n",
        "data['month'] = data['V1'].apply(lambda x: x.month)\n",
        "data['day'] = data['V1'].apply(lambda x: x.day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmFW-CMv_RhN"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# def remove_outliers(df, column_name):\n",
        "#     # Calculate the first quartile (Q1) and third quartile (Q3)\n",
        "#     Q1 = df['V6'].quantile(0.25)\n",
        "#     Q3 = df['V6'].quantile(0.75)\n",
        "\n",
        "#     # Calculate the interquartile range (IQR)\n",
        "#     IQR = Q3 - Q1\n",
        "\n",
        "#     # Define the lower and upper bounds for outliers detection\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "#     # Remove rows where the column value is an outlier\n",
        "#     cleaned_df = df[(df['V6'] >= lower_bound) & (df['V6'] <= upper_bound)]\n",
        "\n",
        "#     return cleaned_df\n",
        "\n",
        "# # Example usage:\n",
        "# # Sample DataFrame (replace this with your own DataFrame)\n",
        "\n",
        "\n",
        "# # Remove rows with outliers in column 'B'\n",
        "# cleaned_df = remove_outliers(df, 'V6')\n",
        "\n",
        "# print(\"Original DataFrame:\")\n",
        "# print(df)\n",
        "# print(\"\\nCleaned DataFrame (without rows containing outliers in column 'B'):\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeK4hnrnAm9r",
        "outputId": "797cfb87-f647-4b9c-85be-738b7fcf27be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[38.34]\n",
            " [42.54]\n",
            " [37.68]\n",
            " ...\n",
            " [47.22]\n",
            " [50.76]\n",
            " [40.86]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data['V6'] = pd.to_numeric(data['V6'], errors='coerce')\n",
        "data = data.dropna(subset=['V6'])\n",
        "\n",
        "dataset = data.V6.values.astype('float32')\n",
        "#Reshape the numpy array into a 2D array with 1 column\n",
        "\n",
        "dataset = np.reshape(dataset, (-1, 1))\n",
        "#Create an instance of the MinMaxScaler class to scale the values between 0 and 1\n",
        "print(dataset)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "#Fit the MinMaxScaler to the transformed data and transform the values\n",
        "\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "#Split the transformed data into a training set (80%) and a test set (20%)\n",
        "\n",
        "\n",
        "train_size = int(len(dataset) * 0.80)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "# train_size = int(len(dataset) * 0.70)  # 70% for training\n",
        "# val_size = int(len(dataset) * 0.15)    # 15% for validation\n",
        "# test_size = len(dataset) - train_size - val_size  # Remaining for test\n",
        "\n",
        "# train, val, test = dataset[0:train_size,:], dataset[train_size:train_size+val_size,:], dataset[train_size+val_size:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-oXjXFO9MnM",
        "outputId": "846418a8-ea3f-40b5-e032-e24a122940f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.62643313]\n",
            " [0.6933121 ]\n",
            " [0.6159236 ]\n",
            " ...\n",
            " [0.7678344 ]\n",
            " [0.8242038 ]\n",
            " [0.66656053]]\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C81_dPPU82nN"
      },
      "outputs": [],
      "source": [
        "col_dates = data.V1.values\n",
        "col_dates = np.reshape(col_dates, (-1, 1))\n",
        "date_train, date_test = col_dates[0:train_size, :], col_dates[train_size:len(dataset), :]\n",
        "\n",
        "# col_dates = data.V1.values\n",
        "# col_dates = np.reshape(col_dates, (-1, 1))\n",
        "# date_train, date_val, date_test = col_dates[0:train_size, :], col_dates[train_size:train_size+val_size,:], col_dates[train_size+val_size:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlOSJ8j1BdfG"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, dates, look_back=1):\n",
        "    X, Y = [], []\n",
        "    d = []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + look_back, 0])\n",
        "        d.append(dates[i + look_back, 0])\n",
        "    return np.array(X), np.array(Y), np.array(d)\n",
        "\n",
        "look_back = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, Y_train, d_train = create_dataset(train, date_train, look_back)\n",
        "# X_val, Y_val, d_val = create_dataset(val, date_val, look_back)\n",
        "X_test, Y_test, d_test = create_dataset(test, date_test, look_back)\n",
        "\n",
        "X_train2, Y_train2, d_train2 = create_dataset(train, date_train, look_back)\n",
        "X_test2, Y_test2, d_test2 = create_dataset(test, date_test, look_back)\n",
        "\n",
        "X_train3, Y_train3, d_train3 = X_train, Y_train, d_train\n",
        "X_test3, Y_test3, d_test3 = X_test, Y_test, d_test\n",
        "\n",
        "X_train4, Y_train4, d_train4 = X_train, Y_train, d_train\n",
        "# X_val4, Y_val4, d_val4 = X_val, Y_val, d_val\n",
        "X_test4, Y_test4, d_test4 = X_test, Y_test, d_test\n",
        "\n",
        "X_train5, Y_train5, d_train5 = X_train, Y_train, d_train\n",
        "X_test5, Y_test5, d_test5 = X_test, Y_test, d_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6sTZZpwNxLs",
        "outputId": "d8333e55-19b4-4286-9c61-f0adf1f03624"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28077,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj5I-PfeBlP6",
        "outputId": "5a0a3653-8aad-41ba-c04d-c29e47a8dddf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28077, 1, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfjAHSiKOUTr",
        "outputId": "789fe67e-1f00-4353-b1c2-97be096d6307",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.62643313 0.6933121  0.6159236  ... 0.04458599 0.04840764 0.04649682]]\n",
            "\n",
            " [[0.6933121  0.6159236  0.6369427  ... 0.04840764 0.04649682 0.04840764]]\n",
            "\n",
            " [[0.6159236  0.6369427  0.6388535  ... 0.04649682 0.04840764 0.04840764]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.70191085 0.6490446  0.7101911  ... 0.6732484  0.5433121  0.7974523 ]]\n",
            "\n",
            " [[0.6490446  0.7101911  0.7133758  ... 0.5433121  0.7974523  0.8910828 ]]\n",
            "\n",
            " [[0.7101911  0.7133758  0.54522294 ... 0.7974523  0.8910828  0.8394905 ]]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYHYYYN9Btsc"
      },
      "outputs": [],
      "source": [
        "X_train2 = np.reshape(X_train2, (X_train2.shape[0], 1, X_train2.shape[1]))\n",
        "X_test2 = np.reshape(X_test2, (X_test2.shape[0], 1, X_test2.shape[1]))\n",
        "X_train2.shape\n",
        "\n",
        "X_train3 = np.reshape(X_train3, (X_train3.shape[0], 1, X_train3.shape[1]))\n",
        "X_test3 = np.reshape(X_test3, (X_test3.shape[0], 1, X_test3.shape[1]))\n",
        "\n",
        "X_train4 = np.reshape(X_train4, (X_train4.shape[0], 1, X_train4.shape[1]))\n",
        "# X_val4 = np.reshape(X_val4, (X_val4.shape[0], 1, X_val4.shape[1]))\n",
        "X_test4 = np.reshape(X_test4, (X_test4.shape[0], 1, X_test4.shape[1]))\n",
        "\n",
        "X_train5 = np.reshape(X_train5, (X_train5.shape[0], 1, X_train5.shape[1]))\n",
        "X_test5 = np.reshape(X_test5, (X_test5.shape[0], 1, X_test5.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc6edfc6Gk2j",
        "outputId": "16920d4a-fc66-4c62-8080-523874f38626"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28077, 1, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_jclkx4Y_Pk",
        "outputId": "df359e9a-fa46-4701-b3bb-6a34814ab3c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tcn\n",
            "  Downloading keras_tcn-3.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-tcn) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-tcn) (2.9.1)\n",
            "Collecting tensorflow-addons (from keras-tcn)\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.11.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.16.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->keras-tcn)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.2.2)\n",
            "Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons, keras-tcn\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-tcn-3.5.0 tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdYiYJcSj6N",
        "outputId": "df8a05b5-a654-4ed2-fd22-dce2b51a30ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train2: (28077, 1, 30)\n",
            "Shape of Y_train2: (28077,)\n",
            "Shape of X_test2: (6997, 1, 30)\n",
            "Shape of Y_test2: (6997,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of X_train2:\", X_train2.shape)\n",
        "print(\"Shape of Y_train2:\", Y_train2.shape)\n",
        "print(\"Shape of X_test2:\", X_test2.shape)\n",
        "print(\"Shape of Y_test2:\", Y_test2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiZ_2PasVDzx",
        "outputId": "139944ee-0239-45ec-a2ef-57322452065f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.62643313, 0.6933121 , 0.6159236 , ..., 0.04458599,\n",
              "         0.04840764, 0.04649682]],\n",
              "\n",
              "       [[0.6933121 , 0.6159236 , 0.6369427 , ..., 0.04840764,\n",
              "         0.04649682, 0.04840764]],\n",
              "\n",
              "       [[0.6159236 , 0.6369427 , 0.6388535 , ..., 0.04649682,\n",
              "         0.04840764, 0.04840764]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.70191085, 0.6490446 , 0.7101911 , ..., 0.6732484 ,\n",
              "         0.5433121 , 0.7974523 ]],\n",
              "\n",
              "       [[0.6490446 , 0.7101911 , 0.7133758 , ..., 0.5433121 ,\n",
              "         0.7974523 , 0.8910828 ]],\n",
              "\n",
              "       [[0.7101911 , 0.7133758 , 0.54522294, ..., 0.7974523 ,\n",
              "         0.8910828 , 0.8394905 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X_train3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXEMhsEZLPW6",
        "outputId": "7fa22d4c-b4b1-4db4-e306-0520c7d5a21a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adabelief-tf==0.2.0\n",
            "  Downloading adabelief_tf-0.2.0-py3-none-any.whl.metadata (622 bytes)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-tf==0.2.0) (2.9.1)\n",
            "Collecting colorama>=0.4.0 (from adabelief-tf==0.2.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-tf==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.11.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.2.2)\n",
            "Downloading adabelief_tf-0.2.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, adabelief-tf\n",
            "Successfully installed adabelief-tf-0.2.0 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install adabelief-tf==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7eMkw-S-qxh",
        "outputId": "42183108-b64b-4e26-ee2d-ceed5f7ee3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 54s 109ms/step - loss: 0.1230 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0067 - val_loss: 0.0082 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 6s 55ms/step - loss: 0.0055 - val_loss: 0.0080 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0053 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 6s 58ms/step - loss: 0.0052 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 8s 77ms/step - loss: 0.0052 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 7s 59ms/step - loss: 0.0051 - val_loss: 0.0078 - lr: 2.0000e-04\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 8s 73ms/step - loss: 0.0050 - val_loss: 0.0079 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 10s 86ms/step - loss: 0.0050 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 10s 90ms/step - loss: 0.0050 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 1, 64)        5824        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 1, 128)       48640       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 1, 128)      36864       ['input_2[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1, 64)        12352       ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 128)         98816       ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 128)         74496       ['bidirectional_2[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 64)          0           ['conv1d_1[0][0]']               \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           8256        ['bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           8256        ['bidirectional_3[0][0]']        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           4160        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 64)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 64)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 64)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 192)          0           ['leaky_re_lu[0][0]',            \n",
            "                                                                  'leaky_re_lu_1[0][0]',          \n",
            "                                                                  'leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 1, 192)       0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, 1, 192)       0           ['reshape[0][0]',                \n",
            "                                                                  'reshape[0][0]']                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 192)         0           ['attention[0][0]']              \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          24704       ['global_max_pooling1d_1[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)      (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           8256        ['leaky_re_lu_4[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 64)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 32)           2080        ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)      (None, 32)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1)            33          ['leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 332,737\n",
            "Trainable params: 332,737\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "219/219 [==============================] - 2s 3ms/step\n",
            "219/219 [==============================] - 2s 3ms/step\n",
            "219/219 [==============================] - 1s 2ms/step\n",
            "219/219 [==============================] - 6s 9ms/step\n",
            "878/878 [==============================] - 3s 3ms/step\n",
            "878/878 [==============================] - 2s 3ms/step\n",
            "878/878 [==============================] - 2s 2ms/step\n",
            "878/878 [==============================] - 5s 6ms/step\n",
            "Test Mean Absolute Error: 4.1276894\n",
            "Test Root Mean Squared Error: 6.0683293\n",
            "Mean Absolute Error for combined model predictions: 3.7854931\n",
            "Root Mean Squared Error for combined model predictions: 5.5447626\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.layers import Concatenate, Dense, LSTM, GRU, Bidirectional, Input, Conv1D, GlobalMaxPooling1D, LeakyReLU, Dropout, Attention, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from adabelief_tf import AdaBeliefOptimizer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from tcn import TCN\n",
        "from tensorflow.keras.layers import Input, Concatenate, Dense, LeakyReLU, Bidirectional, LSTM, Attention, GlobalMaxPooling1D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "lstm_units = 64\n",
        "gru_units = 64\n",
        "tcn_filters = 64\n",
        "tcn_kernel_size = 3\n",
        "initial_learning_rate = 0.001\n",
        "leaky_relu_alpha = 0.01  # Alpha value for LeakyReLU\n",
        "num_heads = 4\n",
        "\n",
        "# Define the LSTM model\n",
        "def create_lstm_model(input_shape, units=lstm_units):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
        "    x = Bidirectional(LSTM(units))(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Define the GRU model\n",
        "def create_gru_model(input_shape, units=gru_units):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(GRU(units, return_sequences=True))(inputs)\n",
        "    x = Bidirectional(GRU(units))(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Define the TCN model\n",
        "def create_tcn_model(input_shape, filters=tcn_filters, kernel_size=tcn_kernel_size):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for dilation_rate in [4, 8]:\n",
        "        x = Conv1D(filters, kernel_size, activation='relu', padding='causal', dilation_rate=dilation_rate)(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Create instances of the LSTM, GRU, and TCN models\n",
        "input_shape_lstm = (X_train5.shape[1], X_train5.shape[2])\n",
        "input_shape_gru = (X_train5.shape[1], X_train5.shape[2])\n",
        "input_shape_tcn = (X_train5.shape[1], X_train5.shape[2])\n",
        "\n",
        "lstm_model = create_lstm_model(input_shape_lstm)\n",
        "gru_model = create_gru_model(input_shape_gru)\n",
        "tcn_model_instance = create_tcn_model(input_shape_tcn)\n",
        "\n",
        "# Define the combined model\n",
        "def combine_models(lstm_model, gru_model, tcn_model):\n",
        "    # Concatenate the outputs of the three models\n",
        "    combined_output = Concatenate()([lstm_model.output, gru_model.output, tcn_model.output])\n",
        "\n",
        "    # Reshape combined_output to have three dimensions if necessary\n",
        "    combined_output = Reshape((1, combined_output.shape[1]))(combined_output)\n",
        "\n",
        "    # Apply attention mechanism\n",
        "    attention = Attention()([combined_output, combined_output])\n",
        "\n",
        "    # Global max pooling to reduce dimensionality\n",
        "    attention = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    x = Dense(256)(attention)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "\n",
        "    x = Dense(128)(attention)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(32)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    combined_model = Model(inputs=[lstm_model.input, gru_model.input, tcn_model.input], outputs=x)\n",
        "    return combined_model\n",
        "\n",
        "\n",
        "combined_model = combine_models(lstm_model, gru_model, tcn_model_instance)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Compile the combined model with AdaBelief optimizer\n",
        "combined_model.compile(loss='mean_squared_error', optimizer=AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True))\n",
        "\n",
        "# Train the combined model with learning rate scheduler\n",
        "history_combined = combined_model.fit([X_train5, X_train5, X_train5], Y_train5, epochs=20, batch_size=256,\n",
        "                                      validation_data=([X_test5, X_test5, X_test5], Y_test5),\n",
        "                                      callbacks=[EarlyStopping(monitor='val_loss', patience=4),\n",
        "                                                 reduce_lr],\n",
        "                                      verbose=1, shuffle=True)\n",
        "\n",
        "# Display model summary\n",
        "combined_model.summary()\n",
        "\n",
        "# Predictions\n",
        "lstm_predictions_test = lstm_model.predict(X_test5)\n",
        "gru_predictions_test = gru_model.predict(X_test5)\n",
        "tcn_predictions_test = tcn_model_instance.predict(X_test5)\n",
        "comb_test = combined_model.predict([X_test5, X_test5, X_test5])\n",
        "\n",
        "lstm_predictions_train = lstm_model.predict(X_train5)\n",
        "gru_predictions_train = gru_model.predict(X_train5)\n",
        "tcn_predictions_train = tcn_model_instance.predict(X_train5)\n",
        "comb_train = combined_model.predict([X_train5, X_train5, X_train5])\n",
        "\n",
        "# Combine predictions with original test features\n",
        "combined_features_test = np.concatenate([X_test5.reshape(X_test5.shape[0], -1), comb_test, lstm_predictions_test, gru_predictions_test, tcn_predictions_test], axis=1)\n",
        "combined_features_train = np.concatenate([X_train5.reshape(X_train5.shape[0], -1), comb_train, lstm_predictions_train, gru_predictions_train, tcn_predictions_train], axis=1)\n",
        "\n",
        "# Define XGBRegressor model\n",
        "xgb_model = XGBRegressor()\n",
        "\n",
        "# Train XGBRegressor on combined features\n",
        "xgb_model.fit(combined_features_train, Y_train5)\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = xgb_model.predict(combined_features_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "test_predictions_inv2 = scaler.inverse_transform(test_predictions.reshape(-1, 1))  # Reshape predictions to match scaler dimensions\n",
        "Y_test5_inv = scaler.inverse_transform(Y_test5.reshape(-1, 1))  # Reshape actual values to match scaler dimensions\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_mae = mean_absolute_error(Y_test5_inv, test_predictions_inv2)\n",
        "test_rmse = np.sqrt(mean_squared_error(Y_test5_inv, test_predictions_inv2))\n",
        "\n",
        "print('Test Mean Absolute Error:', test_mae)\n",
        "print('Test Root Mean Squared Error:', test_rmse)\n",
        "\n",
        "comb_test_inv2 = scaler.inverse_transform(comb_test)\n",
        "\n",
        "tm = mean_absolute_error(Y_test5_inv, comb_test_inv2)\n",
        "print('Mean Absolute Error for combined model predictions:', tm)\n",
        "tr = np.sqrt(mean_squared_error(Y_test5_inv, comb_test_inv2))\n",
        "print('Root Mean Squared Error for combined model predictions:', tr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.layers import Concatenate, Dense, LSTM, GRU, Bidirectional, Input, Conv1D, GlobalMaxPooling1D, LeakyReLU, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from adabelief_tf import AdaBeliefOptimizer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from tcn import TCN\n",
        "\n",
        "# Define hyperparameters\n",
        "lstm_units = 64\n",
        "gru_units = 64\n",
        "tcn_filters = 64\n",
        "tcn_kernel_size = 3\n",
        "initial_learning_rate = 0.001\n",
        "leaky_relu_alpha = 0.01\n",
        "num_heads = 4\n",
        "d_model = 128  # Dimensionality of attention heads\n",
        "\n",
        "# Temporal Multi-Head Attention Class\n",
        "class TemporalMultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super(TemporalMultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Define linear layers for query, key, and value\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "\n",
        "        # Output linear layer\n",
        "        self.dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Split the last dimension into (num_heads, depth)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value = inputs\n",
        "\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Linear transformation and split into heads\n",
        "        query = self.split_heads(self.wq(query), batch_size)\n",
        "        key = self.split_heads(self.wk(key), batch_size)\n",
        "        value = self.split_heads(self.wv(value), batch_size)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        weights = tf.nn.softmax(score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "\n",
        "        # Concatenate heads\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Apply final linear layer\n",
        "        output = self.dense(output)\n",
        "        return output\n",
        "\n",
        "# Define the LSTM model with Temporal Multi-Head Attention\n",
        "def create_lstm_model(input_shape, units=lstm_units):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
        "    x = Bidirectional(LSTM(units))(x)\n",
        "    # Apply Temporal Multi-Head Attention\n",
        "    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])\n",
        "\n",
        "    x = GlobalMaxPooling1D()(attention)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Define the GRU model with Temporal Multi-Head Attention\n",
        "def create_gru_model(input_shape, units=gru_units):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(GRU(units, return_sequences=True))(inputs)\n",
        "    x = Bidirectional(GRU(units))(x)\n",
        "    # Apply Temporal Multi-Head Attention\n",
        "    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])\n",
        "\n",
        "    x = GlobalMaxPooling1D()(attention)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Define the TCN model with Temporal Multi-Head Attention\n",
        "def create_tcn_model(input_shape, filters=tcn_filters, kernel_size=tcn_kernel_size):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for dilation_rate in [4, 8]:\n",
        "        x = Conv1D(filters, kernel_size, activation='relu', padding='causal', dilation_rate=dilation_rate)(x)\n",
        "\n",
        "    # Apply Temporal Multi-Head Attention\n",
        "    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])\n",
        "\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Create instances of the LSTM, GRU, and TCN models\n",
        "input_shape_lstm = (X_train5.shape[1], X_train5.shape[2])\n",
        "input_shape_gru = (X_train5.shape[1], X_train5.shape[2])\n",
        "input_shape_tcn = (X_train5.shape[1], X_train5.shape[2])\n",
        "\n",
        "lstm_model = create_lstm_model(input_shape_lstm)\n",
        "gru_model = create_gru_model(input_shape_gru)\n",
        "tcn_model_instance = create_tcn_model(input_shape_tcn)\n",
        "\n",
        "# Combine the models\n",
        "def combine_models(lstm_model, gru_model, tcn_model):\n",
        "    # Concatenate the outputs of the three models\n",
        "    combined_output = Concatenate()([lstm_model.output, gru_model.output, tcn_model.output])\n",
        "\n",
        "    # Reshape combined_output to have three dimensions if necessary\n",
        "    combined_output = Reshape((1, combined_output.shape[1]))(combined_output)\n",
        "\n",
        "    # Apply temporal multi-head attention on the combined output\n",
        "    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=combined_output.shape[-1])(inputs=[combined_output, combined_output, combined_output])\n",
        "\n",
        "    # Global max pooling to reduce dimensionality\n",
        "    attention = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    x = Dense(256)(attention)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(32)(x)\n",
        "    x = LeakyReLU(alpha=leaky_relu_alpha)(x)\n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    combined_model = Model(inputs=[lstm_model.input, gru_model.input, tcn_model.input], outputs=x)\n",
        "    return combined_model\n",
        "\n",
        "# Now compile and train the combined model\n",
        "combined_model = combine_models(lstm_model, gru_model, tcn_model_instance)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Compile the model with AdaBelief optimizer\n",
        "combined_model.compile(loss='mean_squared_error', optimizer=AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True))\n",
        "\n",
        "# Train the combined model\n",
        "history_combined = combined_model.fit([X_train5, X_train5, X_train5], Y_train5, epochs=20, batch_size=256,\n",
        "                                      validation_data=([X_test5, X_test5, X_test5], Y_test5),\n",
        "                                      callbacks=[EarlyStopping(monitor='val_loss', patience=4), ReduceLROnPlateau()],\n",
        "                                      verbose=1, shuffle=True)\n",
        "\n",
        "# Display model summary\n",
        "combined_model.summary()\n",
        "\n",
        "# Make predictions and evaluate the models\n",
        "lstm_predictions_test = lstm_model.predict(X_test5)\n",
        "gru_predictions_test = gru_model.predict(X_test5)\n",
        "tcn_predictions_test = tcn_model_instance.predict(X_test5)\n",
        "comb_test = combined_model.predict([X_test5, X_test5, X_test5])\n",
        "\n",
        "# Continue with combining features and training XGBoost as in your original code...\n",
        "lstm_predictions_train = lstm_model.predict(X_train5)\n",
        "gru_predictions_train = gru_model.predict(X_train5)\n",
        "tcn_predictions_train = tcn_model_instance.predict(X_train5)\n",
        "comb_train = combined_model.predict([X_train5, X_train5, X_train5])\n",
        "\n",
        "# Combine predictions with original test features\n",
        "combined_features_test = np.concatenate([X_test5.reshape(X_test5.shape[0], -1), comb_test, lstm_predictions_test, gru_predictions_test, tcn_predictions_test], axis=1)\n",
        "combined_features_train = np.concatenate([X_train5.reshape(X_train5.shape[0], -1), comb_train, lstm_predictions_train, gru_predictions_train, tcn_predictions_train], axis=1)\n",
        "\n",
        "# Define XGBRegressor model\n",
        "xgb_model = XGBRegressor()\n",
        "\n",
        "# Train XGBRegressor on combined features\n",
        "xgb_model.fit(combined_features_train, Y_train5)\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = xgb_model.predict(combined_features_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "test_predictions_inv22 = scaler.inverse_transform(test_predictions.reshape(-1, 1))  # Reshape predictions to match scaler dimensions\n",
        "Y_test5_invv = scaler.inverse_transform(Y_test5.reshape(-1, 1))  # Reshape actual values to match scaler dimensions\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_mae = mean_absolute_error(Y_test5_invv, test_predictions_inv22)\n",
        "test_rmse = np.sqrt(mean_squared_error(Y_test5_invv, test_predictions_inv22))\n",
        "\n",
        "print('Test Mean Absolute Error:', test_mae)\n",
        "print('Test Root Mean Squared Error:', test_rmse)\n",
        "\n",
        "comb_test_inv22 = scaler.inverse_transform(comb_test)\n",
        "\n",
        "tm = mean_absolute_error(Y_test5_invv, comb_test_inv22)\n",
        "print('Mean Absolute Error for combined model predictions:', tm)\n",
        "tr = np.sqrt(mean_squared_error(Y_test5_invv, comb_test_inv22))\n",
        "print('Root Mean Squared Error for combined model predictions:', tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVYPnK2f7EkV",
        "outputId": "9568fc7a-8bd8-4c57-8da8-0e94ad5c4411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 72s 136ms/step - loss: 0.0549 - val_loss: 0.0092 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0058 - val_loss: 0.0080 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 12s 109ms/step - loss: 0.0053 - val_loss: 0.0079 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 12s 109ms/step - loss: 0.0053 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0052 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.0053 - val_loss: 0.0078 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 13s 115ms/step - loss: 0.0053 - val_loss: 0.0079 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 12s 107ms/step - loss: 0.0052 - val_loss: 0.0085 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 12s 106ms/step - loss: 0.0053 - val_loss: 0.0080 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0052 - val_loss: 0.0078 - lr: 0.0010\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 1, 128)      48640       ['input_4[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_6 (Bidirectional  (None, 1, 128)      36864       ['input_5[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 128)         98816       ['bidirectional_4[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_7 (Bidirectional  (None, 128)         74496       ['bidirectional_6[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 1, 64)        5824        ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " temporal_multi_head_attention   (None, None, 128)   66048       ['bidirectional_5[0][0]',        \n",
            " (TemporalMultiHeadAttention)                                     'bidirectional_5[0][0]',        \n",
            "                                                                  'bidirectional_5[0][0]']        \n",
            "                                                                                                  \n",
            " temporal_multi_head_attention_  (None, None, 128)   66048       ['bidirectional_7[0][0]',        \n",
            " 1 (TemporalMultiHeadAttention)                                   'bidirectional_7[0][0]',        \n",
            "                                                                  'bidirectional_7[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1, 64)        12352       ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 128)         0           ['temporal_multi_head_attention[0\n",
            " MaxPooling1D)                                                   ][0]']                           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 128)         0           ['temporal_multi_head_attention_1\n",
            " MaxPooling1D)                                                   [0][0]']                         \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 64)          0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 64)           8256        ['global_max_pooling1d_2[0][0]'] \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 64)           8256        ['global_max_pooling1d_3[0][0]'] \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 64)           4160        ['global_max_pooling1d_4[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 64)           0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 64)           0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 64)           0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 192)          0           ['leaky_re_lu_7[0][0]',          \n",
            "                                                                  'leaky_re_lu_8[0][0]',          \n",
            "                                                                  'leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 1, 192)       0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " temporal_multi_head_attention_  (None, None, 192)   148224      ['reshape_1[0][0]',              \n",
            " 3 (TemporalMultiHeadAttention)                                   'reshape_1[0][0]',              \n",
            "                                                                  'reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 192)         0           ['temporal_multi_head_attention_3\n",
            " MaxPooling1D)                                                   [0][0]']                         \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 256)          49408       ['global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_10 (LeakyReLU)     (None, 256)          0           ['dense_27[0][0]']               \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 128)          32896       ['leaky_re_lu_10[0][0]']         \n",
            "                                                                                                  \n",
            " leaky_re_lu_11 (LeakyReLU)     (None, 128)          0           ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 64)           8256        ['leaky_re_lu_11[0][0]']         \n",
            "                                                                                                  \n",
            " leaky_re_lu_12 (LeakyReLU)     (None, 64)           0           ['dense_29[0][0]']               \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 32)           2080        ['leaky_re_lu_12[0][0]']         \n",
            "                                                                                                  \n",
            " leaky_re_lu_13 (LeakyReLU)     (None, 32)           0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 1)            33          ['leaky_re_lu_13[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 670,657\n",
            "Trainable params: 670,657\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "219/219 [==============================] - 3s 3ms/step\n",
            "219/219 [==============================] - 4s 3ms/step\n",
            "219/219 [==============================] - 1s 2ms/step\n",
            "219/219 [==============================] - 7s 7ms/step\n",
            "878/878 [==============================] - 3s 4ms/step\n",
            "878/878 [==============================] - 3s 4ms/step\n",
            "878/878 [==============================] - 2s 3ms/step\n",
            "878/878 [==============================] - 6s 7ms/step\n",
            "Test Mean Absolute Error: 4.034924\n",
            "Test Root Mean Squared Error: 5.9126525\n",
            "Mean Absolute Error for combined model predictions: 3.7823565\n",
            "Root Mean Squared Error for combined model predictions: 5.547778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLBA4at5pf4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d44187b-2763-4322-ff19-047acc70515e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Mean Absolute Error: 4.034924\n",
            "Test Root Mean Squared Error: 5.9126525\n",
            "3.7823565\n",
            "5.547778\n"
          ]
        }
      ],
      "source": [
        "test_predictions_inv = scaler.inverse_transform(test_predictions.reshape(-1, 1))  # Reshape predictions to match scaler dimensions\n",
        "Y_test4_inv = scaler.inverse_transform(Y_test4.reshape(-1, 1))  # Reshape actual values to match scaler dimensions\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_mae = mean_absolute_error(Y_test4_inv, test_predictions_inv)\n",
        "test_rmse = np.sqrt(mean_squared_error(Y_test4_inv, test_predictions_inv))\n",
        "\n",
        "print('Test Mean Absolute Error:', test_mae)\n",
        "print('Test Root Mean Squared Error:', test_rmse)\n",
        "\n",
        "comb_test_inv = scaler.inverse_transform(comb_test)\n",
        "\n",
        "tm = mean_absolute_error(Y_test4_inv, comb_test_inv)\n",
        "print(tm)\n",
        "tr = np.sqrt(mean_squared_error(Y_test4_inv, comb_test_inv))\n",
        "print(tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRHYWinBkLJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c5dc91-f117-4a27-beb9-eef069edc856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-tcn) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-tcn) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from keras-tcn) (0.23.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.11.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-tcn) (1.16.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->keras-tcn) (2.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiBvLu0qmzFu"
      },
      "outputs": [],
      "source": [
        "# train_predict5 = tcn_model.predict(X_train5)\n",
        "# test_predict5 = tcn_model.predict(X_test5)\n",
        "# # invert predictions\n",
        "# train_predict5= scaler.inverse_transform(train_predict5)\n",
        "# Y_train5_inv = scaler.inverse_transform([Y_train5])\n",
        "# test_predict5 = scaler.inverse_transform(test_predict5)\n",
        "# Y_test5_inv = scaler.inverse_transform([Y_test5])\n",
        "\n",
        "# print('Train Mean Absolute Error:', mean_absolute_error(Y_train5_inv[0], train_predict5[:,0]))\n",
        "# print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train5_inv[0], train_predict5[:,0])))\n",
        "# print('Test Mean Absolute Error:', mean_absolute_error(Y_test5_inv[0], test_predict5[:,0]))\n",
        "# print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test5_inv[0], test_predict5[:,0])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJyomUyw4SRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04570ef-86a4-4d2e-b152-a04163618e1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6997, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "lstm_predictions_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xemsLVnjZg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02729543-28a0-40a6-e6a8-fbfca4a194dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 1, 30)]      0           []                               \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 30, 1)        0           ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 30, 128)      1152        ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 30, 128)     512         ['conv1d_4[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 30, 128)      0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 30, 256)      164096      ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 30, 256)     1024        ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 30, 256)      0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 30, 128)      98432       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 30, 128)     512         ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " gru_4 (GRU)                    (None, 8)            960         ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 30, 128)      0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 8)            0           ['gru_4[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['activation_2[0][0]']           \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 136)          0           ['dropout[0][0]',                \n",
            "                                                                  'global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 1)            137         ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 266,825\n",
            "Trainable params: 265,801\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            " 44/351 [==>...........................] - ETA: 1:20 - loss: 0.1723 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d42110e9f956>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mhistory5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Predict on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GRU, Dropout, Conv1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, Permute, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have the following dataset shapes:\n",
        "# X_train5.shape = (num_samples, 1, MAX_SEQUENCE_LENGTH)\n",
        "# Y_train5.shape = (num_samples, NB_CLASS)\n",
        "# X_test5.shape = (num_samples, 1, MAX_SEQUENCE_LENGTH)\n",
        "# Y_test5.shape = (num_samples, NB_CLASS)\n",
        "\n",
        "# Parameters\n",
        "MAX_SEQUENCE_LENGTH = X_train5.shape[2]\n",
        "NB_CLASS = Y_train5.shape[1] if len(Y_train5.shape) > 1 else 1\n",
        "\n",
        "def generate_GRU_FCN_model(input_shape, nb_class):\n",
        "    inp = Input(shape=input_shape)\n",
        "\n",
        "    # GRU part\n",
        "    x_r = GRU(8)(inp)  # GRU with 8 units\n",
        "    x_r = Dropout(0.8)(x_r)  # 80% dropout\n",
        "\n",
        "    # Convolutional part\n",
        "    y = Permute((2, 1))(inp)\n",
        "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)  # 128 filters\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)  # 256 filters\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)  # 128 filters\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    y = GlobalAveragePooling1D()(y)\n",
        "\n",
        "    x = concatenate([x_r, y])\n",
        "\n",
        "    output = Dense(nb_class, activation='softmax' if nb_class > 1 else 'linear')(x)\n",
        "\n",
        "    model = Model(inp, output)\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (1, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Generate the model\n",
        "model5 = generate_GRU_FCN_model(input_shape, NB_CLASS)\n",
        "\n",
        "# Compile the model\n",
        "model5.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error' if NB_CLASS == 1 else 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history5 = model5.fit(X_train5, Y_train5, epochs=20, batch_size=64, validation_split=0.2, verbose=1, shuffle=True)\n",
        "\n",
        "# Predict on test set\n",
        "predictions5 = model5.predict(X_test5)\n",
        "\n",
        "# If it's a regression task, you may want to calculate metrics like MAE and RMSE\n",
        "if NB_CLASS == 1:\n",
        "    test_mae = mean_absolute_error(Y_test5, predictions5)\n",
        "    test_rmse = np.sqrt(mean_squared_error(Y_test5, predictions5))\n",
        "    print('Test Mean Absolute Error:', test_mae)\n",
        "    print('Test Root Mean Squared Error:', test_rmse)\n",
        "else:\n",
        "    # For classification tasks, you might use accuracy or other classification metrics\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    test_acc = accuracy_score(np.argmax(Y_test5, axis=1), np.argmax(predictions5, axis=1))\n",
        "    print('Test Accuracy:', test_acc)\n",
        "predictions5_inv = scaler.inverse_transform(predictions5)\n",
        "Y_test5_inv = scaler.inverse_transform([Y_test5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3g5TbSqjiPS"
      },
      "outputs": [],
      "source": [
        "grufcn_mae = mean_absolute_error(Y_test5_inv[0], predictions5_inv[:,0])\n",
        "grufcn_rmse = np.sqrt(mean_squared_error(Y_test5_inv[0], predictions5_inv[:,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2f3Yj6jwqxc"
      },
      "outputs": [],
      "source": [
        "# Y_test = scaler.inverse_transform([Y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLOeKDiHmSct"
      },
      "outputs": [],
      "source": [
        "# !pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IABmmBagNC_9"
      },
      "outputs": [],
      "source": [
        "# print(\"Shape of Y_train:\", Y_train.shape)\n",
        "# print(\"Shape of Y_test:\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAXl-ioGLR65"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import *\n",
        "\n",
        "# train_predict = model(X_train)\n",
        "# test_predict = model(X_test)\n",
        "# #invert predictions\n",
        "# train_predict = scaler.inverse_transform(train_predict)\n",
        "# Y_train = scaler.inverse_transform([Y_train])\n",
        "# test_predict = scaler.inverse_transform(test_predict)\n",
        "# Y_test = scaler.inverse_transform([Y_test])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))\n",
        "# print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))\n",
        "# print('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\n",
        "# print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3ArMvslUfOD"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Convert predictions tensor to NumPy array\n",
        "# predictions_np = predictions.detach().numpy()\n",
        "\n",
        "# # Convert Y_test tensor to NumPy array if it's not already in NumPy format\n",
        "# Y_test_np = Y_test.numpy()  # Replace Y_test with your actual test data\n",
        "\n",
        "# # Plot actual vs predicted values\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(Y_test_np, label='Actual')\n",
        "# plt.plot(predictions_np, label='Predicted')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('Value')\n",
        "# plt.title('Actual vs Predicted')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gvUVliJcW1B"
      },
      "outputs": [],
      "source": [
        "train_predict1 = model1.predict(X_train2)\n",
        "test_predict1 = model1.predict(X_test2)\n",
        "# invert predictions\n",
        "train_predict1 = scaler.inverse_transform(train_predict1)\n",
        "Y_train2 = scaler.inverse_transform([Y_train2])\n",
        "test_predict1 = scaler.inverse_transform(test_predict1)\n",
        "Y_test2 = scaler.inverse_transform([Y_test2])\n",
        "\n",
        "mae_bilstm = mean_absolute_error(Y_test2[0], test_predict1[:,0])\n",
        "rmse_bilstm = np.sqrt(mean_squared_error(Y_test2[0], test_predict1[:,0]))\n",
        "\n",
        "print('Train Mean Absolute Error:', mean_absolute_error(Y_train2[0], train_predict1[:,0]))\n",
        "print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train2[0], train_predict1[:,0])))\n",
        "print('Test Mean Absolute Error:', mean_absolute_error(Y_test2[0], test_predict1[:,0]))\n",
        "print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test2[0], test_predict1[:,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxBlZGUupYnO"
      },
      "outputs": [],
      "source": [
        "train_predict2 = model4.predict(X_train3)\n",
        "test_predict2 = model4.predict(X_test3)\n",
        "# invert predictions\n",
        "train_predict2 = scaler.inverse_transform(train_predict2)\n",
        "Y_train3 = scaler.inverse_transform([Y_train3])\n",
        "test_predict2 = scaler.inverse_transform(test_predict2)\n",
        "Y_test3 = scaler.inverse_transform([Y_test3])\n",
        "\n",
        "mae_Rgru = mean_absolute_error(Y_test3[0], test_predict2[:,0])\n",
        "rmse_Rgru = np.sqrt(mean_squared_error(Y_test3[0], test_predict1[:,0]))\n",
        "\n",
        "print('Train Mean Absolute Error:', mean_absolute_error(Y_train3[0], train_predict2[:,0]))\n",
        "print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train3[0], train_predict2[:,0])))\n",
        "print('Test Mean Absolute Error:', mean_absolute_error(Y_test3[0], test_predict2[:,0]))\n",
        "print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test3[0], test_predict2[:,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yj6hUYmkn6_"
      },
      "outputs": [],
      "source": [
        "rgru_mae = mean_absolute_error(Y_test3[0], test_predict2[:,0])\n",
        "rgru_rmse = np.sqrt(mean_squared_error(Y_test3[0], test_predict2[:,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANpXmLtHnWau"
      },
      "outputs": [],
      "source": [
        "# train_predict3 = model3.predict(X_train4)\n",
        "# test_predict3 = model3.predict(X_test4)\n",
        "# # invert predictions\n",
        "# train_predict3 = scaler.inverse_transform(train_predict3)\n",
        "# Y_train4 = scaler.inverse_transform([Y_train4])\n",
        "# test_predict3 = scaler.inverse_transform(test_predict3)\n",
        "# Y_test4 = scaler.inverse_transform([Y_test4])\n",
        "\n",
        "# print('Train Mean Absolute Error:', mean_absolute_error(Y_train4[0], train_predict3[:,0]))\n",
        "# print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train4[0], train_predict3[:,0])))\n",
        "# print('Test Mean Absolute Error:', mean_absolute_error(Y_test4[0], test_predict3[:,0]))\n",
        "# print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test4[0], test_predict3[:,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo8HazJKk2Z6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses_bilstm = history2.history['loss']\n",
        "val_losses_bilstm = history2.history['val_loss']\n",
        "train_losses_cnngru = history_cnn_gru.history['loss']\n",
        "val_losses_cnngru = history_cnn_gru.history['val_loss']\n",
        "train_losses_rgru = history4.history['loss']\n",
        "val_losses_rgru = history4.history['val_loss']\n",
        "train_losses_grufcn = history5.history['loss']\n",
        "val_losses_grufcn = history5.history['val_loss']\n",
        "\n",
        "train_losses_hybrid = history_combined.history['loss']\n",
        "val_losses_hybrid = history_combined.history['val_loss']\n",
        "\n",
        "\n",
        "#epochs_lstm = len(history.history['val_loss'])\n",
        "epochs_bilstm = len(history2.history['val_loss'])\n",
        "epochs_cnngru = len(history_cnn_gru.history['val_loss'])\n",
        "epochs_rgru = len(history4.history['val_loss'])\n",
        "epochs_hybrid = len(history_combined.history['val_loss'])\n",
        "epochs_grufcn = len(history5.history['val_loss'])\n",
        "\n",
        "# Plot the training and validation loss for each model\n",
        "\n",
        "plt.plot(range(epochs_bilstm), val_losses_bilstm, label='BiLSTM Loss', marker='o')\n",
        "plt.plot(range(epochs_cnngru), val_losses_cnngru, label='CNNGRU Loss', marker='s')\n",
        "plt.plot(range(epochs_rgru), val_losses_rgru, label='R.GRU Loss', marker='^')\n",
        "plt.plot(range(epochs_grufcn), val_losses_grufcn, label='GRUFCN Loss', marker='o')\n",
        "plt.plot(range(epochs_hybrid), val_losses_hybrid, label='Hybrid Loss', marker='s')\n",
        "\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss of Different Models')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4hI8C56mnfn"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "aa=[x for x in range(50)]\n",
        "# Creating a figure object with desired figure size\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "# Plotting the actual values in blue with a dot marker\n",
        "plt.plot(aa, Y_test[:,0][100:150], marker='.', linewidth=0.9,label=\"actual\", color='black')\n",
        "\n",
        "# Plotting the predicted values in green with a solid line\n",
        "plt.plot(aa, test_predict[:, 0][100:150], '.-', label=\"CNNGRU prediction\", color='#ec9b00', linewidth=0.5)\n",
        "plt.plot(aa, test_predict1[:, 0][100:150], '.-', linewidth=0.5, label=\"BiLSTM prediction\", color='green')\n",
        "# plt.plot(aa, test_predict3[:, 0][:100], '.-', label=\"GRU prediction\", color='purple', linewidth=0.7)\n",
        "plt.plot(aa, test_predict2[:, 0][100:150], '.-', label=\"Regularised GRU prediction\", color='blue', linewidth=0.5)\n",
        "plt.plot(aa, test_predictions_inv[100:150], '.-', label=\"hybrid\", color='red', linewidth=0.9)\n",
        "plt.plot(aa, predictions5_inv[100:150], '.-', label=\"GRUFCN\", color='purple', linewidth=0.5)\n",
        "# Removing the top spines\n",
        "sns.despine(top=True)\n",
        "\n",
        "# Adjusting the subplot location\n",
        "plt.subplots_adjust(left=0.07)\n",
        "\n",
        "# Labeling the y-axis\n",
        "plt.ylabel('Global_active_power', size=14)\n",
        "\n",
        "# Labeling the x-axis\n",
        "plt.xlabel('Time step', size=14)\n",
        "\n",
        "# Adding a legend with font size of 15\n",
        "plt.legend(fontsize=16)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlo5Mqlibf94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses_bilstm = history2.history['loss']\n",
        "val_losses_bilstm = history2.history['val_loss']\n",
        "# train_losses_gru = history2.history['loss']\n",
        "# val_losses_gru = history2.history['val_loss']\n",
        "train_losses_rgru = history4.history['loss']\n",
        "val_losses_rgru = history4.history['val_loss']\n",
        "\n",
        "train_losses_hybrid = history_combined.history['loss']\n",
        "val_losses_hybrid = history_combined.history['val_loss']\n",
        "\n",
        "#epochs_lstm = len(history.history['val_loss'])\n",
        "epochs_bilstm = len(history2.history['val_loss'])\n",
        "# epochs_gru = len(history2.history['val_loss'])\n",
        "epochs_rgru = len(history4.history['val_loss'])\n",
        "epochs_hybrid = len(history_combined.history['val_loss'])\n",
        "\n",
        "plt.plot(range(epochs_bilstm), train_losses_bilstm, label='BiLSTM Loss', marker='o')\n",
        "# plt.plot(range(epochs_gru), train_losses_gru, label='GRU Loss', marker='s')\n",
        "plt.plot(range(epochs_rgru), train_losses_rgru, label='R.GRU Loss', marker='^')\n",
        "# plt.plot(range(epochs_lstm), train_losses_lstm, label='LSTM Loss', marker='o')\n",
        "plt.plot(range(epochs_hybrid), train_losses_hybrid, label='Hybrid Loss', marker='s')\n",
        "\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss of Different Models')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI8q6VBtf2Pe"
      },
      "outputs": [],
      "source": [
        "# Y_test = scaler.inverse_transform([Y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPNnH9jwb66K"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# aa=[x for x in range(40)]\n",
        "# # Creating a figure object with desired figure size\n",
        "# plt.figure(figsize=(20,6))\n",
        "\n",
        "# # Plotting the actual values in blue with a dot marker\n",
        "# plt.plot(aa, Y_test[0][:40], marker='.', label=\"actual\", color='black')\n",
        "\n",
        "# # Plotting the predicted values in green with a solid line\n",
        "# # plt.plot(aa, test_predict[:, 0][:100], '.-', label=\"LSTM prediction\", color='red', linewidth=0.5)\n",
        "# plt.plot(aa, test_predict1[:, 0][:40], '.-', linewidth=1.0, label=\"BiLSTM prediction\", color='green')\n",
        "# # plt.plot(aa, test_predict3[:, 0][:100], '.-', label=\"GRU prediction\", color='purple', linewidth=0.7)\n",
        "# plt.plot(aa, test_predict2[:, 0][:40], '.-', label=\"Regularised GRU prediction\", color='blue', linewidth=0.5)\n",
        "# plt.plot(aa, comb_test_inv[:40], '.-', label=\"hybrid\", color='red', linewidth=0.5)\n",
        "# # Removing the top spines\n",
        "# sns.despine(top=True)\n",
        "\n",
        "# # Adjusting the subplot location\n",
        "# plt.subplots_adjust(left=0.07)\n",
        "\n",
        "# # Labeling the y-axis\n",
        "# plt.ylabel('Global_active_power', size=14)\n",
        "\n",
        "# # Labeling the x-axis\n",
        "# plt.xlabel('Time step', size=14)\n",
        "\n",
        "# # Adding a legend with font size of 15\n",
        "# plt.legend(fontsize=16)\n",
        "\n",
        "# # Display the plot\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJKBIS2uOSL2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_error_table(Y_test, test_predict, dates):\n",
        "    error_table = {}\n",
        "    date_count = {}\n",
        "\n",
        "    for date, actual, predicted in zip(dates, Y_test, test_predict):\n",
        "        absolute_error = np.abs(actual - predicted[0])\n",
        "        if actual > 0:\n",
        "            error = (absolute_error / actual) * 100\n",
        "        else:\n",
        "            error = 0  # Handling case where actual value is zero\n",
        "        if date not in date_count:\n",
        "            date_count[date] = 0\n",
        "            error_table[date] = 0\n",
        "        date_count[date] += 1\n",
        "        error_table[date] += error\n",
        "\n",
        "    for date in error_table:\n",
        "        error_table[date] /= date_count[date]\n",
        "\n",
        "    return error_table\n",
        "\n",
        "# Example usage\n",
        "# error_percentages_lstm = calculate_error_table(Y_test[0], test_predict, d_test)\n",
        "#error_percentages_bilstm = calculate_error_table(Y_test2[0], test_predict1, d_test2)\n",
        "#error_percentages_Regularised_gru = calculate_error_table(Y_test3[0], test_predict2, d_test3)\n",
        "#error_percentages_hybrid = calculate_error_table(Y_test4_inv, comb_test_inv, d_test4)\n",
        "# error_percentages_lstm = calculate_error_table(Y_test, test_predict, d_test)\n",
        "# error_percentages_bilstm = calculate_error_table(Y_test2[0], test_predict1, d_test2)\n",
        "# error_percentages_Regularised_gru = calculate_error_table(Y_test3[0], test_predict2, d_test3)\n",
        "# error_percentages_hybrid = calculate_error_table(Y_test4_inv, comb_test_inv, d_test4)\n",
        "error_percentages_hybrid2 = calculate_error_table(Y_test5_inv, comb_test_inv2, d_test5)\n",
        "error_percentages_hybrid3 = calculate_error_table(Y_test5_invv, comb_test_inv22, d_test5)\n",
        "# error_percentages_GRUFCN = calculate_error_table(Y_test5_inv[0], predictions5_inv, d_test5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCmI3pNWWa6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c13ea7db-b3c2-4206-eb89-6408238a4322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Hy Hybrid_percentage\n",
              "0    [7.3364053]       [7.3389688]\n",
              "1   [10.3258505]       [10.340236]\n",
              "2     [8.934412]        [8.819829]\n",
              "3     [9.406438]        [9.522425]\n",
              "4    [11.361874]       [11.393669]\n",
              "5    [6.1662135]       [6.2385025]\n",
              "6     [9.375335]        [9.254388]\n",
              "7     [8.038418]        [8.092538]\n",
              "8     [8.833605]        [8.829265]\n",
              "9    [10.533647]       [10.503636]\n",
              "10   [10.085531]        [10.12182]\n",
              "11    [9.713479]        [9.877151]\n",
              "12   [7.9163475]        [7.854794]\n",
              "13   [7.6925206]       [7.7141457]\n",
              "14    [7.363974]       [7.3418107]\n",
              "15   [10.951741]        [11.00418]\n",
              "16    [8.455468]        [8.133784]\n",
              "17    [7.632015]        [7.731657]\n",
              "18   [6.6730294]       [6.6861343]\n",
              "19    [7.328584]        [7.318529]\n",
              "20    [6.852419]       [6.8210187]\n",
              "21   [7.8578095]       [7.7835255]\n",
              "22    [9.224773]        [9.247943]\n",
              "23    [9.506625]        [9.538474]\n",
              "24    [8.480429]        [8.430606]\n",
              "25   [6.4794693]       [6.5237675]\n",
              "26   [5.4104877]        [5.492369]\n",
              "27    [5.298945]       [5.2601166]\n",
              "28    [7.670321]       [7.6780934]\n",
              "29   [10.475161]       [10.300765]\n",
              "30    [9.326533]        [9.504177]\n",
              "31    [8.042925]        [8.049324]\n",
              "32    [8.247664]       [8.3213825]\n",
              "33     [8.01361]       [7.9540257]\n",
              "34   [6.9720683]       [6.9290886]\n",
              "35    [8.634891]        [8.590177]\n",
              "36   [12.775711]       [12.784358]\n",
              "37   [10.527651]       [10.454917]\n",
              "38   [7.0333314]        [7.058739]\n",
              "39   [7.9483743]        [7.983276]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df7e02b3-ad63-4bb8-bcc3-e9713b6acc8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hy</th>\n",
              "      <th>Hybrid_percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[7.3364053]</td>\n",
              "      <td>[7.3389688]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[10.3258505]</td>\n",
              "      <td>[10.340236]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[8.934412]</td>\n",
              "      <td>[8.819829]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[9.406438]</td>\n",
              "      <td>[9.522425]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[11.361874]</td>\n",
              "      <td>[11.393669]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[6.1662135]</td>\n",
              "      <td>[6.2385025]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[9.375335]</td>\n",
              "      <td>[9.254388]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[8.038418]</td>\n",
              "      <td>[8.092538]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[8.833605]</td>\n",
              "      <td>[8.829265]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[10.533647]</td>\n",
              "      <td>[10.503636]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[10.085531]</td>\n",
              "      <td>[10.12182]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[9.713479]</td>\n",
              "      <td>[9.877151]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[7.9163475]</td>\n",
              "      <td>[7.854794]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[7.6925206]</td>\n",
              "      <td>[7.7141457]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[7.363974]</td>\n",
              "      <td>[7.3418107]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[10.951741]</td>\n",
              "      <td>[11.00418]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[8.455468]</td>\n",
              "      <td>[8.133784]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[7.632015]</td>\n",
              "      <td>[7.731657]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[6.6730294]</td>\n",
              "      <td>[6.6861343]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[7.328584]</td>\n",
              "      <td>[7.318529]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[6.852419]</td>\n",
              "      <td>[6.8210187]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[7.8578095]</td>\n",
              "      <td>[7.7835255]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[9.224773]</td>\n",
              "      <td>[9.247943]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[9.506625]</td>\n",
              "      <td>[9.538474]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[8.480429]</td>\n",
              "      <td>[8.430606]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[6.4794693]</td>\n",
              "      <td>[6.5237675]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[5.4104877]</td>\n",
              "      <td>[5.492369]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[5.298945]</td>\n",
              "      <td>[5.2601166]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[7.670321]</td>\n",
              "      <td>[7.6780934]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[10.475161]</td>\n",
              "      <td>[10.300765]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[9.326533]</td>\n",
              "      <td>[9.504177]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[8.042925]</td>\n",
              "      <td>[8.049324]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[8.247664]</td>\n",
              "      <td>[8.3213825]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[8.01361]</td>\n",
              "      <td>[7.9540257]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[6.9720683]</td>\n",
              "      <td>[6.9290886]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[8.634891]</td>\n",
              "      <td>[8.590177]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[12.775711]</td>\n",
              "      <td>[12.784358]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[10.527651]</td>\n",
              "      <td>[10.454917]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[7.0333314]</td>\n",
              "      <td>[7.058739]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[7.9483743]</td>\n",
              "      <td>[7.983276]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df7e02b3-ad63-4bb8-bcc3-e9713b6acc8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df7e02b3-ad63-4bb8-bcc3-e9713b6acc8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df7e02b3-ad63-4bb8-bcc3-e9713b6acc8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8231a651-f5ef-4fac-9448-d7d5404bac0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8231a651-f5ef-4fac-9448-d7d5404bac0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8231a651-f5ef-4fac-9448-d7d5404bac0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "percentage_df",
              "summary": "{\n  \"name\": \"percentage_df\",\n  \"rows\": 73,\n  \"fields\": [\n    {\n      \"column\": \"Hy\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hybrid_percentage\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# percentage_df = pd.DataFrame(zip(error_percentages_lstm.keys(), error_percentages_lstm.values(), error_percentages_bilstm.values(), error_percentages_gru.values(), error_percentages_Regularised_gru.values()), columns=[\"Date\", \"LSTM(%)\", \"BiLSTM(%)\", \"GRU(%)\", \"Reg.GRU(%)\"])\n",
        "percentage_df = pd.DataFrame(zip(error_percentages_hybrid2.values(), error_percentages_hybrid3.values()), columns=[\"Hy\", \"Hybrid_percentage\"])\n",
        "\n",
        "percentage_df.head(40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_error_hybrid3 = percentage_df[\"Hybrid_percentage\"].mean()\n",
        "average_error_hybrid2 = percentage_df[\"Hy\"].mean()\n",
        "print(average_error_hybrid3)\n",
        "print(average_error_hybrid2)"
      ],
      "metadata": {
        "id": "JkStO-WRw3z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6033412-4003-43ae-aec4-1447bc3c7ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.136634]\n",
            "[9.249881]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZiprgc4njT7"
      },
      "outputs": [],
      "source": [
        "average_error_bilstm = percentage_df[\"BiLSTM\"].mean()\n",
        "average_error_cnngru = percentage_df[\"CNNGRU(%)\"].mean()\n",
        "average_error_Reg_gru = percentage_df[\"Reg.GRU(%)\"].mean()\n",
        "average_error_hybrid = percentage_df[\"Hybrid_percentage\"].mean()\n",
        "average_error_GRUFCN = 111\n",
        "# max_value = 100  # Replace with the actual maximum value if known\n",
        "# average_error_lstm = (percentage_df[\"LSTM(%)\"] / max_value).mean()\n",
        "# average_error_bilstm = (percentage_df[\"BiLSTM(%)\"] / max_value).mean()\n",
        "# average_error_gru = (percentage_df[\"GRU(%)\"] / max_value).mean()\n",
        "\n",
        "\n",
        "# print(average_error_lstm)\n",
        "print(average_error_bilstm)\n",
        "print(average_error_cnngru)\n",
        "print(average_error_Reg_gru)\n",
        "print(average_error_hybrid)\n",
        "# print(average_error_GRUFCN)\n",
        "# Find the method with the lowest average error\n",
        "best_method = None\n",
        "lowest_error = min(average_error_bilstm, average_error_hybrid, average_error_Reg_gru, average_error_cnngru)\n",
        "if lowest_error == average_error_bilstm:\n",
        " best_method = \"BiLSTM\"\n",
        "elif lowest_error == average_error_hybrid:\n",
        " best_method = \"hybrid\"\n",
        "elif lowest_error == average_error_cnngru:\n",
        " best_method = \"CNNGRU\"\n",
        "else:\n",
        " best_method = \"RegGRU\"\n",
        "\n",
        "# Print the DataFrame with additional information\n",
        "print(percentage_df)\n",
        "print(f\"\\nOverall Best Method: {best_method} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsnQHhJkEP0x"
      },
      "outputs": [],
      "source": [
        "# average_error_lstm = percentage_df[\"LSTM(%)\"].mean()\n",
        "average_error_bilstm = percentage_df[\"BiLSTM\"].mean()\n",
        "# average_error_gru = percentage_df[\"GRU(%)\"].mean()\n",
        "average_error_Reg_gru = percentage_df[\"Reg.GRU(%)\"].mean()\n",
        "average_error_hybrid = percentage_df[\"Hybrid(%)\"].mean()\n",
        "# max_value = 100  # Replace with the actual maximum value if known\n",
        "# average_error_lstm = (percentage_df[\"LSTM(%)\"] / max_value).mean()\n",
        "# average_error_bilstm = (percentage_df[\"BiLSTM(%)\"] / max_value).mean()\n",
        "# average_error_gru = (percentage_df[\"GRU(%)\"] / max_value).mean()\n",
        "\n",
        "\n",
        "# print(average_error_lstm)\n",
        "print(average_error_bilstm)\n",
        "# print(average_error_gru)\n",
        "print(average_error_Reg_gru)\n",
        "print(average_error_hybrid)\n",
        "# Find the method with the lowest average error\n",
        "best_method = None\n",
        "lowest_error = min( average_error_bilstm, average_error_hybrid, average_error_Reg_gru)\n",
        "if lowest_error == average_error_bilstm:\n",
        " best_method = \"BiLSTM\"\n",
        "elif lowest_error == average_error_hybrid:\n",
        " best_method = \"hybrid\"\n",
        "else:\n",
        " best_method = \"RegGRU\"\n",
        "\n",
        "# Print the DataFrame with additional information\n",
        "print(percentage_df)\n",
        "print(f\"\\nOverall Best Method: {best_method} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQtDzwZ3ERC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Define models and their corresponding RMSE and MAE values\n",
        "model_names = ['BiLSTM', 'R. GRU', 'Hybrid']  # Add your model names here\n",
        "test_rmsed = [rmse_bilstm, rmse_Rgru, test_rmse]  # RMSE values for Model A, Model B, and Model C\n",
        "test_maed = [mae_bilstm, mae_Rgru, test_mae]   # MAE values for Model A, Model B, and Model C\n",
        "\n",
        "# Create a figure and axis\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Set bar colors using a colormap\n",
        "colors = plt.cm.tab10(np.arange(len(model_names)))\n",
        "colors = ('r','g','b')\n",
        "\n",
        "# Define positions for bars\n",
        "x_pos = np.arange(len(model_names))\n",
        "y_pos_rmse = np.ones(len(model_names)) * 0\n",
        "y_pos_mae = np.ones(len(model_names)) * 1\n",
        "\n",
        "bar_width = 0.2  # Decrease bar width for better separation\n",
        "\n",
        "# Plot RMSE bars\n",
        "rmse_bars = ax.bar3d(x_pos - bar_width/2, y_pos_rmse, np.zeros(len(model_names)), bar_width, bar_width, test_rmsed, color=colors, alpha=0.8)\n",
        "\n",
        "# Plot MAE bars\n",
        "mae_bars = ax.bar3d(x_pos + bar_width/2, y_pos_mae, np.zeros(len(model_names)), bar_width, bar_width, test_maed, color=colors, alpha=0.8)\n",
        "\n",
        "# Set tick labels and axis labels\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_yticklabels(['RMSE', 'MAE'])\n",
        "ax.set_zlabel('Values', fontsize=12)\n",
        "ax.set_title('RMSE and MAE Comparison of Different Models', fontsize=14)\n",
        "\n",
        "# Set axis limits\n",
        "ax.set_xlim(-0.5, len(model_names) - 0.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.set_zlim(0, max(max(test_rmsed), max(test_maed)) * 1.1)\n",
        "\n",
        "# Adjust lighting\n",
        "ax.view_init(elev=20, azim=30)\n",
        "\n",
        "# Create custom legend\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors[i]) for i in range(len(model_names))]\n",
        "ax.legend(handles, model_names, loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSDmf9a1Ve5J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some sample data\n",
        "actual_data = Y_test[0][:600] # Actual data\n",
        "model1_predictions = test_predict1[:, 0][:600]  # Predictions of Model 1\n",
        "model2_predictions = test_predict2[:, 0][:600]  # Predictions of Model 2\n",
        "model3_predictions = test_predictions_inv[:600]  # Predictions of Model 3\n",
        "\n",
        "# Define time periods (in days)\n",
        "weekly_periods = 7\n",
        "monthly_periods = 30\n",
        "yearly_periods = 365\n",
        "daily_periods = 1\n",
        "\n",
        "actual_data_daily = actual_data[:len(actual_data)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "actual_data_weekly = actual_data[:len(actual_data)//weekly_periods * weekly_periods].reshape(-1, weekly_periods).mean(axis=1)\n",
        "actual_data_monthly = actual_data[:len(actual_data)//monthly_periods * monthly_periods].reshape(-1, monthly_periods).mean(axis=1)\n",
        "actual_data_yearly = actual_data[:len(actual_data)//yearly_periods * yearly_periods].reshape(-1, yearly_periods).mean(axis=1)\n",
        "\n",
        "model1_predictions_daily = model1_predictions[:len(model1_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model2_predictions_daily = model2_predictions[:len(model2_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model3_predictions_daily = model3_predictions[:len(model3_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "\n",
        "# Reshape predictions for different models and time periods\n",
        "model1_predictions_weekly = model1_predictions[:len(model1_predictions)//weekly_periods * weekly_periods].reshape(-1, weekly_periods).mean(axis=1)\n",
        "model2_predictions_weekly = model2_predictions[:len(model2_predictions)//weekly_periods * weekly_periods].reshape(-1, weekly_periods).mean(axis=1)\n",
        "model3_predictions_weekly = model3_predictions[:len(model3_predictions)//weekly_periods * weekly_periods].reshape(-1, weekly_periods).mean(axis=1)\n",
        "\n",
        "model1_predictions_monthly = model1_predictions[:len(model1_predictions)//monthly_periods * monthly_periods].reshape(-1, monthly_periods).mean(axis=1)\n",
        "model2_predictions_monthly = model2_predictions[:len(model2_predictions)//monthly_periods * monthly_periods].reshape(-1, monthly_periods).mean(axis=1)\n",
        "model3_predictions_monthly = model3_predictions[:len(model3_predictions)//monthly_periods * monthly_periods].reshape(-1, monthly_periods).mean(axis=1)\n",
        "\n",
        "model1_predictions_yearly = model1_predictions[:len(model1_predictions)//yearly_periods * yearly_periods].reshape(-1, yearly_periods).mean(axis=1)\n",
        "model2_predictions_yearly = model2_predictions[:len(model2_predictions)//yearly_periods * yearly_periods].reshape(-1, yearly_periods).mean(axis=1)\n",
        "model3_predictions_yearly = model3_predictions[:len(model3_predictions)//yearly_periods * yearly_periods].reshape(-1, yearly_periods).mean(axis=1)\n",
        "\n",
        "# Create scatter plots\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "\n",
        "plt.scatter(actual_data_weekly, model1_predictions_weekly, label='BiLSTM')\n",
        "plt.scatter(actual_data_weekly, model2_predictions_weekly, label='R.GRU')\n",
        "plt.scatter(actual_data_weekly, model3_predictions_weekly, label='Hybrid')\n",
        "plt.plot(actual_data_weekly, actual_data_weekly, color='k', linestyle='-.', label='Actual Data')\n",
        "plt.xlabel('Actual Data (Weekly)')\n",
        "plt.ylabel('Predicted Data')\n",
        "plt.title('Weekly Performance Comparison')\n",
        "plt.legend()\n",
        "\n",
        "# plt.subplot(3, 1, 2)\n",
        "# plt.scatter(actual_data_monthly, model1_predictions_monthly, label='BiLSTM')\n",
        "# plt.scatter(actual_data_monthly, model2_predictions_monthly, label='R.GRU')\n",
        "# plt.scatter(actual_data_monthly, model3_predictions_monthly, label='Hybrid')\n",
        "# plt.plot(actual_data_monthly, actual_data_monthly, color='k', linestyle='--', label='Actual Data')\n",
        "# plt.xlabel('Actual Data (Monthly)')\n",
        "# plt.ylabel('Predicted Data')\n",
        "# plt.title('Monthly Performance Comparison')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(3, 1, 3)\n",
        "# plt.scatter(actual_data_yearly, model1_predictions_yearly, label='BiLSTM')\n",
        "# plt.scatter(actual_data_yearly, model2_predictions_yearly, label='R.GRU')\n",
        "# plt.scatter(actual_data_yearly, model3_predictions_yearly, label='Hybrid')\n",
        "# plt.plot(actual_data_yearly, actual_data_yearly, color='k', linestyle='--', label='Actual Data')\n",
        "# plt.xlabel('Actual Data (Yearly)')\n",
        "# plt.ylabel('Predicted Data')\n",
        "# plt.title('Yearly Performance Comparison')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AfTVpH9Yph0"
      },
      "outputs": [],
      "source": [
        "plt.scatter(actual_data_monthly, model1_predictions_monthly, label='BiLSTM')\n",
        "plt.scatter(actual_data_monthly, model2_predictions_monthly, label='R.GRU')\n",
        "plt.scatter(actual_data_monthly, model3_predictions_monthly, label='Hybrid')\n",
        "plt.plot(actual_data_monthly, actual_data_monthly, color='k', linestyle='--', label='Actual Data')\n",
        "plt.xlabel('Actual Data (Monthly)')\n",
        "plt.ylabel('Predicted Data')\n",
        "plt.title('Monthly Performance Comparison')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTkWFnQ9dHKZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCPOPtKYZzek"
      },
      "outputs": [],
      "source": [
        "actual_data = Y_test[0][:100] # Actual data\n",
        "model1_predictions = test_predict1[:, 0][:100]  # Predictions of Model 1\n",
        "model2_predictions = test_predict2[:, 0][:100]  # Predictions of Model 2\n",
        "model3_predictions = test_predictions_inv[:100]  # Predictions of Model 3\n",
        "\n",
        "# Define time periods (in days)\n",
        "weekly_periods = 7\n",
        "monthly_periods = 30\n",
        "yearly_periods = 365\n",
        "daily_periods = 1\n",
        "\n",
        "actual_data_daily = actual_data[:len(actual_data)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model1_predictions_daily = model1_predictions[:len(model1_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model2_predictions_daily = model2_predictions[:len(model2_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model3_predictions_daily = model3_predictions[:len(model3_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "\n",
        "plt.scatter(actual_data_daily, model1_predictions_daily, label='Model 1')\n",
        "plt.scatter(actual_data_daily, model2_predictions_daily, label='Model 2')\n",
        "plt.scatter(actual_data_daily, model3_predictions_daily, label='Model 3')\n",
        "plt.plot(actual_data_daily, actual_data_daily, color='k', linestyle='--', label='Actual Data')\n",
        "plt.xlabel('Actual Data (Daily)')\n",
        "plt.ylabel('Predicted Data')\n",
        "plt.title('Daily Performance Comparison')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fz-BPz3dIUv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "actual_data = Y_test[0][:100] # Actual data\n",
        "model1_predictions = test_predict1[:, 0][:100]  # Predictions of Model 1\n",
        "model2_predictions = test_predict2[:, 0][:100]  # Predictions of Model 2\n",
        "model3_predictions = test_predictions_inv[:100]  # Predictions of Model 3\n",
        "\n",
        "# Define time periods (in days)\n",
        "weekly_periods = 7\n",
        "monthly_periods = 30\n",
        "yearly_periods = 365\n",
        "daily_periods = 1\n",
        "\n",
        "actual_data_daily = actual_data[:len(actual_data)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "actual_data_weekly = actual_data[:len(actual_data)//weekly_periods * weekly_periods].reshape(-1, weekly_periods).mean(axis=1)\n",
        "actual_data_monthly = actual_data[:len(actual_data)//monthly_periods * monthly_periods].reshape(-1, monthly_periods).mean(axis=1)\n",
        "actual_data_yearly = actual_data[:len(actual_data)//yearly_periods * yearly_periods].reshape(-1, yearly_periods).mean(axis=1)\n",
        "\n",
        "model1_predictions_daily = model1_predictions[:len(model1_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model2_predictions_daily = model2_predictions[:len(model2_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "model3_predictions_daily = model3_predictions[:len(model3_predictions)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "\n",
        "# Reshape data for different time periods\n",
        "actual_data_daily = actual_data[:len(actual_data)//daily_periods * daily_periods].reshape(-1, daily_periods).mean(axis=1)\n",
        "\n",
        "# Get day of the week (0 = Monday, 6 = Sunday)\n",
        "day_of_week = np.arange(len(actual_data)) % 7\n",
        "\n",
        "# Create scatter plots with different markers for weekdays and weekends\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] < 5], model1_predictions[day_of_week[:len(actual_data_daily)] < 5], label='BiLSTM (Weekdays)', marker='o')\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] >= 5], model1_predictions[day_of_week[:len(actual_data_daily)] >= 5], label='BiLSTM (Weekends)', marker='^')\n",
        "\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] < 5], model2_predictions[day_of_week[:len(actual_data_daily)] < 5], label='R_GRU (Weekdays)', marker='o')\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] >= 5], model2_predictions[day_of_week[:len(actual_data_daily)] >= 5], label='R_GRU (Weekends)', marker='^')\n",
        "\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] < 5], model3_predictions[day_of_week[:len(actual_data_daily)] < 5], label='Hybrid (Weekdays)', marker='o')\n",
        "plt.scatter(actual_data_daily[day_of_week[:len(actual_data_daily)] >= 5], model3_predictions[day_of_week[:len(actual_data_daily)] >= 5], label='Hybrid (Weekends)', marker='^')\n",
        "\n",
        "plt.plot(actual_data_daily, actual_data_daily, color='k', linestyle='--', label='Actual Data')\n",
        "\n",
        "plt.xlabel('Actual Data (Daily)')\n",
        "plt.ylabel('Predicted Data')\n",
        "plt.title('Performance Comparison by Weekdays and Weekends')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU0sNpb5lnPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have predictions and actual values from different models\n",
        "# Replace the following arrays with your actual predictions and actual values\n",
        "predictions_model_1 = test_predict1[:, 0][:50]  # Example predictions from Model 1\n",
        "predictions_model_2 = test_predict2[:, 0][:50]  # Example predictions from Model 2\n",
        "predictions_model_3 = test_predictions_inv[:, 0][:50]  # Example predictions from Model 3\n",
        "actual_values = Y_test[0][:50]  # Example actual values\n",
        "\n",
        "# Combine predictions and actual values from different models into a 2D array\n",
        "data = np.vstack((predictions_model_1, predictions_model_2, predictions_model_3, actual_values))\n",
        "\n",
        "# Plot heatmap\n",
        "\n",
        "plt.imshow(data, cmap='viridis', aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Values')\n",
        "\n",
        "# Add labels to y-axis\n",
        "model_names = ['BiLSTM', 'RGRU', 'Hybrid', 'Actual Values']\n",
        "plt.yticks(range(len(model_names)), model_names)\n",
        "\n",
        "plt.xlabel('Timestamps')\n",
        "plt.title('Predictions and Actual Values of Time Series Data from Different Models')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}