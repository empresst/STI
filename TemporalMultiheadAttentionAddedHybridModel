import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.layers import Concatenate, Dense, LSTM, GRU, Bidirectional, Input, Conv1D, GlobalMaxPooling1D, LeakyReLU, Reshape
from tensorflow.keras.models import Model
from adabelief_tf import AdaBeliefOptimizer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor
from tcn import TCN

# Define hyperparameters
lstm_units = 64
gru_units = 64
tcn_filters = 64
tcn_kernel_size = 3
initial_learning_rate = 0.001
leaky_relu_alpha = 0.01
num_heads = 4
d_model = 128  # Dimensionality of attention heads

# Temporal Multi-Head Attention Class
class TemporalMultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, num_heads, d_model):
        super(TemporalMultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.depth = d_model // num_heads

        # Define linear layers for query, key, and value
        self.wq = Dense(d_model)
        self.wk = Dense(d_model)
        self.wv = Dense(d_model)

        # Output linear layer
        self.dense = Dense(d_model)

    def split_heads(self, x, batch_size):
        # Split the last dimension into (num_heads, depth)
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        query, key, value = inputs

        batch_size = tf.shape(query)[0]

        # Linear transformation and split into heads
        query = self.split_heads(self.wq(query), batch_size)
        key = self.split_heads(self.wk(key), batch_size)
        value = self.split_heads(self.wv(value), batch_size)

        # Scaled dot-product attention
        score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.depth, tf.float32))
        weights = tf.nn.softmax(score, axis=-1)
        output = tf.matmul(weights, value)

        # Concatenate heads
        output = tf.transpose(output, perm=[0, 2, 1, 3])
        output = tf.reshape(output, (batch_size, -1, self.d_model))

        # Apply final linear layer
        output = self.dense(output)
        return output

# Define the LSTM model with Temporal Multi-Head Attention
def create_lstm_model(input_shape, units=lstm_units):
    inputs = Input(shape=input_shape)
    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)
    x = Bidirectional(LSTM(units))(x)
    # Apply Temporal Multi-Head Attention
    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])

    x = GlobalMaxPooling1D()(attention)
    x = Dense(64)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    model = Model(inputs=inputs, outputs=x)
    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)
    model.compile(loss='mean_squared_error', optimizer=optimizer)
    return model

# Define the GRU model with Temporal Multi-Head Attention
def create_gru_model(input_shape, units=gru_units):
    inputs = Input(shape=input_shape)
    x = Bidirectional(GRU(units, return_sequences=True))(inputs)
    x = Bidirectional(GRU(units))(x)
    # Apply Temporal Multi-Head Attention
    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])

    x = GlobalMaxPooling1D()(attention)
    x = Dense(64)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    model = Model(inputs=inputs, outputs=x)
    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)
    model.compile(loss='mean_squared_error', optimizer=optimizer)
    return model

# Define the TCN model with Temporal Multi-Head Attention
def create_tcn_model(input_shape, filters=tcn_filters, kernel_size=tcn_kernel_size):
    inputs = Input(shape=input_shape)
    x = inputs
    for dilation_rate in [4, 8]:
        x = Conv1D(filters, kernel_size, activation='relu', padding='causal', dilation_rate=dilation_rate)(x)

    # Apply Temporal Multi-Head Attention
    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=d_model)([x, x, x])

    x = GlobalMaxPooling1D()(x)
    x = Dense(64)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    model = Model(inputs=inputs, outputs=x)
    optimizer = AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True)
    model.compile(loss='mean_squared_error', optimizer=optimizer)
    return model

# Create instances of the LSTM, GRU, and TCN models
input_shape_lstm = (X_train5.shape[1], X_train5.shape[2])
input_shape_gru = (X_train5.shape[1], X_train5.shape[2])
input_shape_tcn = (X_train5.shape[1], X_train5.shape[2])

lstm_model = create_lstm_model(input_shape_lstm)
gru_model = create_gru_model(input_shape_gru)
tcn_model_instance = create_tcn_model(input_shape_tcn)

# Combine the models
def combine_models(lstm_model, gru_model, tcn_model):
    # Concatenate the outputs of the three models
    combined_output = Concatenate()([lstm_model.output, gru_model.output, tcn_model.output])

    # Reshape combined_output to have three dimensions if necessary
    combined_output = Reshape((1, combined_output.shape[1]))(combined_output)

    # Apply temporal multi-head attention on the combined output
    attention = TemporalMultiHeadAttention(num_heads=num_heads, d_model=combined_output.shape[-1])(inputs=[combined_output, combined_output, combined_output])

    # Global max pooling to reduce dimensionality
    attention = GlobalMaxPooling1D()(attention)

    x = Dense(256)(attention)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)

    x = Dense(128)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    x = Dense(64)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    x = Dense(32)(x)
    x = LeakyReLU(alpha=leaky_relu_alpha)(x)
    x = Dense(1)(x)

    combined_model = Model(inputs=[lstm_model.input, gru_model.input, tcn_model.input], outputs=x)
    return combined_model

# Now compile and train the combined model
combined_model = combine_models(lstm_model, gru_model, tcn_model_instance)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)

# Compile the model with AdaBelief optimizer
combined_model.compile(loss='mean_squared_error', optimizer=AdaBeliefOptimizer(learning_rate=initial_learning_rate, epsilon=1e-14, rectify=True))

# Train the combined model
history_combined = combined_model.fit([X_train5, X_train5, X_train5], Y_train5, epochs=20, batch_size=3200,
                                      validation_data=([X_test5, X_test5, X_test5], Y_test5),
                                      callbacks=[EarlyStopping(monitor='val_loss', patience=4), ReduceLROnPlateau()],
                                      verbose=1, shuffle=True)

# Display model summary
combined_model.summary()

# Make predictions and evaluate the models
lstm_predictions_test = lstm_model.predict(X_test5)
gru_predictions_test = gru_model.predict(X_test5)
tcn_predictions_test = tcn_model_instance.predict(X_test5)
comb_test = combined_model.predict([X_test5, X_test5, X_test5])

# Continue with combining features and training XGBoost as in your original code...
lstm_predictions_train = lstm_model.predict(X_train5)
gru_predictions_train = gru_model.predict(X_train5)
tcn_predictions_train = tcn_model_instance.predict(X_train5)
comb_train = combined_model.predict([X_train5, X_train5, X_train5])

# Combine predictions with original test features
combined_features_test = np.concatenate([X_test5.reshape(X_test5.shape[0], -1), comb_test, lstm_predictions_test, gru_predictions_test, tcn_predictions_test], axis=1)
combined_features_train = np.concatenate([X_train5.reshape(X_train5.shape[0], -1), comb_train, lstm_predictions_train, gru_predictions_train, tcn_predictions_train], axis=1)

# Define XGBRegressor model
xgb_model = XGBRegressor()

# Train XGBRegressor on combined features
xgb_model.fit(combined_features_train, Y_train5)

# Make predictions
test_predictions = xgb_model.predict(combined_features_test)

# Inverse transform the predictions and actual values
test_predictions_inv22 = scaler.inverse_transform(test_predictions.reshape(-1, 1))  # Reshape predictions to match scaler dimensions
Y_test5_invv = scaler.inverse_transform(Y_test5.reshape(-1, 1))  # Reshape actual values to match scaler dimensions

# Calculate evaluation metrics
test_mae = mean_absolute_error(Y_test5_invv, test_predictions_inv22)
test_rmse = np.sqrt(mean_squared_error(Y_test5_invv, test_predictions_inv22))

print('Test Mean Absolute Error:', test_mae)
print('Test Root Mean Squared Error:', test_rmse)

comb_test_inv22 = scaler.inverse_transform(comb_test)

tm = mean_absolute_error(Y_test5_invv, comb_test_inv22)
print('Mean Absolute Error for combined model predictions:', tm)
tr = np.sqrt(mean_squared_error(Y_test5_invv, comb_test_inv22))
print('Root Mean Squared Error for combined model predictions:', tr)
